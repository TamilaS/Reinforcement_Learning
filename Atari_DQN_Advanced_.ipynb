{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Atari_DQN_youtube.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOaxNa5AWGwGAsst84sSSJ2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "\n",
        "!pip3 install atari-py\n",
        "!python -m atari_py.import_roms ROMS\n",
        "!pip install \"gym[atari]\" \"gym[accept-rom-license]\" atari_py\n",
        "!pip install -U \"ray[rllib]==1.11\""
      ],
      "metadata": {
        "id": "pP9AEcv3HU_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oUYPan5ag-k"
      },
      "outputs": [],
      "source": [
        "pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch gym\n"
      ],
      "metadata": {
        "id": "EfrF5D1Jarvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # pip install zlib1g-dev cmake\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.8.0 box2d box2d-kengz"
      ],
      "metadata": {
        "id": "wB3omeXPayOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cmake\n"
      ],
      "metadata": {
        "id": "z-sKRDU5bABe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.distributed.rpc\n",
        "from torch import nn\n",
        "import torch\n",
        "import gym\n",
        "from collections import deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from baselines_wrappers import DummyVecEnv, SubprocVecEnv, Monitor\n",
        "from pytorch_wrappers import make_atari_deepmind, BatchedPytorchFrameStack, PytorchLazyFrames\n",
        "import msgpack\n",
        "from msgpack_numpy import patch as msgpack_numpy_patch\n",
        "msgpack_numpy_patch()\n",
        "\n",
        "\n",
        "GAMMA=0.99\n",
        "BATCH_SIZE=32\n",
        "BUFFER_SIZE=int(le6) \n",
        "MIN_REPLAY_SIZE=50000\n",
        "EPSILON_START=1.0\n",
        "EPSILON_END=0.1\n",
        "EPSILON_DECAY=int(le6)\n",
        "TARGET_UPDATE_FREQ = 10000\n",
        "NUM_ENVS = 4 #steps in between gradient steps\n",
        "LR = 2.5e-4\n",
        "SAVE_PATH = './atari_model.pack'\n",
        "SAVE_INTERVAL = 10000\n",
        "LOG_DIR ='./logs/atari'\n",
        "LOG_INTERVAL= 1000\n",
        "\n",
        "def nature_cnn(observastion_space, depths = (32, 64, 64), final_layer =512):  #from the doc Nature journal\n",
        "    n_input_channels = observastion_space.shape[0]\n",
        "\n",
        "    cnn = nn.Sequential(\n",
        "        nn.Conv2d(n_input_channels, depths[0], kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        n_flatten = cnn(torch.as_tensor(observastion_space.sample()[None]).float()).shape[1]  #forward pass through cnn\n",
        "\n",
        "    out = nn.Sequential(cnn, nn.Linear(n_flatten, final_layer), nn.ReLU())\n",
        "    return out\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, env, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.device = device\n",
        "\n",
        "        conv_net = nature_cnn(env.observation_space.shape)\n",
        "\n",
        "        self.net = nn.Sequential(conv_net, nn.Linear(512, self.num_actions))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def act(self, obses , epsilon):\n",
        "        obses_t = torch.as_tensor(obses, dtype=torch.float32, device = self.device)\n",
        "        q_values = self(obses_t)\n",
        "        max_q_indices = torch.argmax(q_values, dim=1)\n",
        "        actions = max_q_indices.detach().tolist()\n",
        "\n",
        "        for i in range(len(actions)): #epsilon greedy policy \n",
        "            rnd_sample = random.random()\n",
        "            if rnd_sample <= epsilon:\n",
        "                actions[i] =random.randint(0, self.num_actions - 1)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def compute_loss(self, transitions, target_net):\n",
        "        obses = [t[0] for t in transitions]\n",
        "        actions = np.asarray([t[1] for t in transitions])\n",
        "        rews = np.asarray([t[2] for t in transitions])\n",
        "        dones = np.asarray([t[3] for t in transitions])\n",
        "        new_obses = [t[4] for t in transitions]\n",
        "\n",
        "        if isinstance(obses[0], PytorchLazyFrames):\n",
        "            obses = np.stack([o.get_frames() for o in obses])  # converts into numpy array\n",
        "            new_obses = np.stack([o.get_frames() for o in obses])\n",
        "        else:\n",
        "            obses = np.asarray(obses)\n",
        "            new_obses = np.asarray(new_obses)\n",
        "\n",
        "\n",
        "        obses_t = torch.as_tensor(obses, dtype=torch.float32, device = self.device)\n",
        "        actions_t = torch.as_tensor(actions, dtype=torch.int64, device = self.device).unsqueeze(-1)\n",
        "        rews_t = torch.as_tensor(rews, dtype=torch.float32, device = self.device).unsqueeze(-1)\n",
        "        dones_t = torch.as_tensor(dones, dtype=torch.float32, device = self.device).unsqueeze(-1)\n",
        "        new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32, device = self.device)\n",
        "\n",
        "        # Compute Targets\n",
        "        # targets = r + gamma * target q vals * (1 - dones)\n",
        "        target_q_values = target_net(new_obses_t)\n",
        "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "\n",
        "        targets = rews_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
        "\n",
        "        # Compute Loss\n",
        "        q_values = self(obses_t)\n",
        "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
        "\n",
        "        loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def save(self, save_path ):\n",
        "        params = {k: t.detach().cpu().numpy() for k,t in self.state_dict().items()}    #datas with numpy array rather than tensors\n",
        "        params_data = msgpack.dumps(params) #serialised our parameter dict \n",
        "\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        with open (save_path, 'wb') as f: #open as binary data\n",
        "            f.write(params_data)\n",
        "\n",
        "    def load(self, load_path):\n",
        "        if not os.path.exists(load_path):\n",
        "            raise FileNotFoundError(load_path)\n",
        "        with open(load_path, 'rb') as f: #open in binary mode\n",
        "            params_numpy = msgpack.loads(f.read())#deserialising\n",
        "\n",
        "        params = {k: torch.as_tensor(v, device = self.device) for k,v in params_numpy.items()} #transfor to tensor\n",
        "        self.load_state_dict(params)#loading\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "make_env =lambda: Monitor(make_atari_deepmind('Breakout-v0'), allow_early_resets = True) #using function from the pytorch wrapper file\n",
        "\n",
        "vec_env = DummyVecEnv([make_env for _ in range(NUM_ENVS)]) #runs env in sequence\n",
        "#env = SubprocVecEnv([make_env for _ in NUM_ENVS]) #each of the env in parallel\n",
        "\n",
        "env = BatchedPytorchFrameStack(vec_env, k=4) #special wrapper of vec_env for the frame stacking\n",
        "\n",
        "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
        "epinfos_buffer = deque([], maxlen=100)\n",
        "\n",
        "episode_count = 0\n",
        "\n",
        "summary_writer = SummaryWriter(LOG_DIR) #included in pytorch\n",
        "\n",
        "online_net = Network(env, device= device)\n",
        "target_net = Network(env, device = device)\n",
        "\n",
        "online_net = online_net.to(device)\n",
        "online_net = target_net.to(device)\n",
        "\n",
        "target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e-4)\n",
        "\n",
        "# Initialize replay buffer\n",
        "obs = env.reset()\n",
        "for _ in range(MIN_REPLAY_SIZE):\n",
        "    actions = [env.action_space.sample() for _ in range(NUM_ENVS)] #one  for each env, multiple actions sampling\n",
        "\n",
        "    new_obses, rews, dones, _ = env.step(actions)\n",
        "    for obs, action, rew, done, new_pbs in zip(obses, actions, rews, dones, new_obses): #zipping components of our transitions with batch dim\n",
        "        transition = (obs, action, rew, done, new_obs) #pulling each ele to construct the single transition separately\n",
        "        replay_buffer.append(transition) #append to replay buffer\n",
        "        obses = new_obses\n",
        "        #after if its done, DummyVecEnv/SubprocVecEnv resets\n",
        "\n",
        "    obses = new_obses\n",
        "# Main Training Loop\n",
        "obses = env.reset()\n",
        "for step in itertools.count():\n",
        "    epsilon = np.interp(step*NUM_ENVS, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
        "\n",
        "    rnd_sample = random.random()\n",
        "\n",
        "    if isinstance(obses[0], PytorchLazyFrames):#from pytorch wrappers\n",
        "        act_obses = np.stack([o.get_frames() for o in obses]) #converts PytorchLazyFrames into numpy array from a list\n",
        "        actions = online_net.act(act_obses, epsilon)\n",
        "    else:\n",
        "        actions = online_net.act(obses, epsilon)\n",
        "\n",
        "    new_obses, rews, dones, infos = env.step(actions)\n",
        "    for obs, action, rew, done, new_obs, info in zip(obses, actions, rews, dones,new_obses, infos):  # zipping components of our transitions with batch dim\n",
        "        transition = (obs, action, rew, done, new_obs)  # pulling each ele to construct the single transition separately\n",
        "        replay_buffer.append(transition)  # append to replay buffer\n",
        "        # after if its done, DummyVecEnv/SubprocVecEnv resets\n",
        "        if done:\n",
        "          epinfos_buffer.append(info['episode'])\n",
        "          episode_count += 1\n",
        "\n",
        "    obses = new_obses\n",
        "\n",
        "    #watch agent play\n",
        "\n",
        "\n",
        "    #gradient desc.\n",
        "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
        "    loss = online_net.compute_loss(transitions, target_net)\n",
        "\n",
        "    # Gradient Descent\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Target Net\n",
        "    if step % TARGET_UPDATE_FREQ == 0:\n",
        "        target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "    # Logging\n",
        "    if step % LOG_INTERVAL == 0:\n",
        "        rew_mean = np.mean([e['r'] for e in epinfos_buffer]) or 0 #mean of all rewards iunside the episode buffer, 0 prevents from having nan values\n",
        "        len_mean = np.mean([e['l '] for e in epinfos_buffer]) or 0 #length of episode\n",
        "        print()\n",
        "        print('Step:', step)\n",
        "        print('Avg rew:', rew_mean)\n",
        "        print('Av Ep Len:', len_)\n",
        "        print('Episodes:', episode_count)\n",
        "\n",
        "        summary_writer.add_scalar('Av reward', rew_mean, global_step=step) #average reward at each step, added to tensorboard graphs\n",
        "        summary_writer.add_scalar('AvEpLen', len_mean, global_step=step) \n",
        "        summary_writer.add_scalar('Episodes', episode_count, global_step=step)\n",
        "    #SAVING\n",
        "    if step % SAVE_INTERVAL == 0 and step !=0:#we dont want to run it when step =0\n",
        "        print('Saving..')\n",
        "        online_net.save(SAVE_PATH)\n",
        "\n"
      ],
      "metadata": {
        "id": "CxMDWlD9bwP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mknGu4v9bo6o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}