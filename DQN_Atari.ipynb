{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Atari_DQN_youtube.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPHAUVDsyKUzgH41Ri7HYUE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "\n",
        "!pip3 install atari-py\n",
        "!python -m atari_py.import_roms ROMS\n",
        "!pip install \"gym[atari]\" \"gym[accept-rom-license]\" atari_py\n",
        "!pip install -U \"ray[rllib]==1.11\""
      ],
      "metadata": {
        "id": "pP9AEcv3HU_U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9oUYPan5ag-k"
      },
      "outputs": [],
      "source": [
        "pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch gym\n"
      ],
      "metadata": {
        "id": "EfrF5D1Jarvy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # pip install zlib1g-dev cmake\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.8.0 box2d box2d-kengz"
      ],
      "metadata": {
        "id": "wB3omeXPayOr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cmake\n"
      ],
      "metadata": {
        "id": "z-sKRDU5bABe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import gym\n",
        "from collections import deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "metadata": {
        "id": "F-hY9BX6bCCU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch.distributed.rpc\n",
        "from torch import nn\n",
        "import torch\n",
        "import gym\n",
        "from collections import deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from baselines_wrappers import DummyVecEnv, SubprocVecEnv, Monitor\n",
        "from pytorch_wrappers import make_atari_deepmind, BatchedPytorchFrameStack, PytorchLazyFrames\n",
        "\n",
        "import msgpack\n",
        "from msgpack_numpy import patch as msgpack_numpy_patch\n",
        "msgpack_numpy_patch()\n",
        "\n",
        "\n",
        "GAMMA=0.99\n",
        "BATCH_SIZE=32\n",
        "BUFFER_SIZE=int(le6) \n",
        "MIN_REPLAY_SIZE=1000\n",
        "EPSILON_START=1.0\n",
        "EPSILON_END=0.1\n",
        "EPSILON_DECAY=int(le6)\n",
        "TARGET_UPDATE_FREQ = 10000\n",
        "NUM_ENVS = 4\n",
        "LR = 2.5e-4\n",
        "SAVE_PATH = './atari_model.pack'\n",
        "SAVE_INTERVAL = 10000\n",
        "LOG_DIR ='./logs/atari'\n",
        "LOG_INTERVAL= 1000\n",
        "\n",
        "def nature_cnn(observastion_space, depths = (32, 64, 64), final_layer =512):  #from the doc Nature journal\n",
        "    n_input_channels = observastion_space.shape[0]\n",
        "\n",
        "    cnn = nn.Sequential(\n",
        "        nn.Conv2d(n_input_channels, depths[0], kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        n_flatten = cnn(torch.as_tensor(observastion_space.sample()[None]).float()).shape[1]  #forward pass through cnn\n",
        "\n",
        "    out = nn.Sequential(cnn, nn.Linear(n_flatten, final_layer), nn.ReLU())\n",
        "    return out\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, env, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.device = device\n",
        "        #in_features = int(np.prod(env.observation_space.shape))  #input is a 3d tensor\n",
        "\n",
        "        conv_net = nature_cnn(env.observation_space.shape)\n",
        "\n",
        "        self.net = nn.Sequential(conv_net, Linear(512, self.num_actions))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def act(self, obses , epsilon):\n",
        "        obs_t = torch.as_tensor(obses, dtype=torch.float32, device = self.device)\n",
        "        q_values = self(obses_t)\n",
        "        max_q_indices = torch.argmax(q_values, dim=1)[0]\n",
        "        actions = max_q_indices.detach().tolist()\n",
        "\n",
        "        for i in range(len(actions)):\n",
        "            rnd_sample = random.random()\n",
        "            if rnd_sample <= epsilon:\n",
        "                actions[i] =random.randint(0, self.num_actions - 1)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def compute_loss(self, transitions, target_net):\n",
        "        obses = [t[0] for t in transitions]\n",
        "        actions = np.asarray([t[1] for t in transitions])\n",
        "        rews = np.asarray([t[2] for t in transitions])\n",
        "        dones = np.asarray([t[3] for t in transitions])\n",
        "        new_obses = [t[4] for t in transitions]\n",
        "\n",
        "        if isinstance(obses[0], PytorchLazyFrames):\n",
        "            obses = np.stack([o.get_frames() for o in obses])  # converts into numpy array\n",
        "            new_obses = np.stack([o.get_frames() for o in obses])\n",
        "        else:\n",
        "            obses = np.asarray(obses)\n",
        "            new_obses = np.asarray(new_obses)\n",
        "\n",
        "\n",
        "        obses_t = torch.as_tensor(obses, dtype=torch.float32, device = self.device)\n",
        "        actions_t = torch.as_tensor(actions, dtype=torch.int64, device = self.device).unsqueeze(-1)\n",
        "        rews_t = torch.as_tensor(rews, dtype=torch.float32, device = self.device).unsqueeze(-1)\n",
        "        dones_t = torch.as_tensor(dones, dtype=torch.float32, device = self.device).unsqueeze(-1)\n",
        "        new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32, device = self.device)\n",
        "\n",
        "        # Compute Targets\n",
        "        # targets = r + gamma * target q vals * (1 - dones)\n",
        "        target_q_values = target_net(new_obses_t)\n",
        "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "\n",
        "        targets = rews_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
        "\n",
        "        # Compute Loss\n",
        "        q_values = self(obses_t)\n",
        "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
        "\n",
        "        loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def save(self, save_path ):\n",
        "        params = {k: t.detach().cpu().numpy() for k,t in self.state_dict().items()}    #datas with numpy array rather than tensors\n",
        "        params_data = msgpack.dumps(params)\n",
        "\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        with open (save_path, 'wb') as f: #open as binary data\n",
        "            f.write(params_data)\n",
        "\n",
        "    def load(self, load_path):\n",
        "        if not os.path.exists(load_path):\n",
        "            raise FileNotFoundError(load_path)\n",
        "        with open(load_path, 'rb') as f:\n",
        "            params_numpy = msgpack.loads(f.read())\n",
        "\n",
        "        params = {k: torch.as_tensor(v, device = self.device) for k,v in params_numpy.items()} #transfor to tensor\n",
        "        self.load_state_dict(params)#loading\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#env = gym.make('CartPole-v0')\n",
        "make_env =lambda: Monitor(make_atari_deepmind('Breakout-v0'), allow_early_resets = True)\n",
        "\n",
        "vec_env = DummyVecEnv([make_env for _ in range(NUM_ENVS)]) #runs env in sequence\n",
        "#env = SubprocVecEnv([make_env for _ in NUM_ENVS]) #each of the env in parallel\n",
        "\n",
        "env = BatchedPytorchFrameStack(vec_env, k=4)\n",
        "\n",
        "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
        "epinfos_buffer = deque([], maxlen=100)\n",
        "\n",
        "episode_count = 0\n",
        "\n",
        "summary_writer = SummaryWriter(LOG_DIR)\n",
        "\n",
        "online_net = Network(env, device= device)\n",
        "target_net = Network(env, device = device)\n",
        "\n",
        "online_net = online_net.to(device)\n",
        "online_net = target_net.to(device)\n",
        "\n",
        "target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e-4)\n",
        "\n",
        "# Initialize replay buffer\n",
        "obs = env.reset()\n",
        "for _ in range(MIN_REPLAY_SIZE):\n",
        "    actions = [env.action_space.sample() for _ in range(NUM_ENVS)] #one  for each env, multiple actions sampling\n",
        "\n",
        "    new_obses, rews, dones, _ = env.step(actions)\n",
        "    for obs, action, rew, done, new_pbs in zip(obses, actions, rews, dones, new_obses): #zipping components of our transitions with batch dim\n",
        "        transition = (obs, action, rew, done, new_obs) #pulling each ele to construct the single transition separately\n",
        "        replay_buffer.append(transition) #append to replay buffer\n",
        "        obses = new_obses\n",
        "        #after if its done, DummyVecEnv/SubprocVecEnv resets\n",
        "\n",
        "    obses = new_obses\n",
        "# Main Training Loop\n",
        "obses = env.reset()\n",
        "for step in itertools.count():\n",
        "    epsilon = np.interp(step*NUM_ENVS, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
        "\n",
        "    rnd_sample = random.random()\n",
        "\n",
        "    if isinstance(obses[0], PytorchLazyFrames):\n",
        "        act_obses = np.stack([o.get_frames() for o in obses]) #converts into numpy array\n",
        "        actions = online_net.act(act_obses, epsilon)\n",
        "    else:\n",
        "        actions = online_net.act(obses, epsilon)\n",
        "\n",
        "    new_obses, rews, dones, infos = env.step(actions)\n",
        "    for obs, action, rew, done, new_obs, info in zip(obses, actions, rews, dones,new_obses, infos):  # zipping components of our transitions with batch dim\n",
        "        transition = (obs, action, rew, done, new_obs)  # pulling each ele to construct the single transition separately\n",
        "        replay_buffer.append(transition)  # append to replay buffer\n",
        "        obses = new_obses\n",
        "        # after if its done, DummyVecEnv/SubprocVecEnv resets\n",
        "\n",
        "        if done:\n",
        "            epinfos_buffer.append(info['episode'])\n",
        "            episode_count += 1\n",
        "\n",
        "    obses = new_obses\n",
        "\n",
        "    #watch agent play\n",
        "\n",
        "\n",
        "    #gradient desc.\n",
        "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
        "    loss = online_net.compute_loss(transitions, target_net)\n",
        "\n",
        "    # Gradient Descent\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Target Net\n",
        "    if step % TARGET_UPDATE_FREQ == 0:\n",
        "        target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "    # Logging\n",
        "    if step % LOG_INTERVAL == 0:\n",
        "        rew_mean = np.mean([e['r'] for e in epinfos_buffer]) or 0 #mean of all rewards iunside the episode buffer\n",
        "        len_mean = np.mean([e['l '] for e in epinfos_buffer]) or 0 #length of episode\n",
        "        print()\n",
        "        print('Step:', step)\n",
        "        print('Avg rew:', rew_mean)\n",
        "        print('Av Ep Len:', len_)\n",
        "        print('Episodes:', episode_count)\n",
        "\n",
        "        summary_writer.add_scalar('Av reward', rew_mean, global_step=step)\n",
        "        summary_writer.add_scalar('AvEpLen', len_mean, global_step=step)\n",
        "        summary_writer.add_scalar('Episodes', episode_count, global_step=step)\n",
        "    #SAVING\n",
        "    if step % SAVE_INTERVAL == 0 and step !=0:\n",
        "        print('Saving..')\n",
        "        online_net.save(SAVE_PATH)\n",
        "\n"
      ],
      "metadata": {
        "id": "CxMDWlD9bwP2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mknGu4v9bo6o"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}